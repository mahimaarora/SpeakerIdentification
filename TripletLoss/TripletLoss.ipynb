{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter, butter\n",
    "import decimal\n",
    "import math\n",
    "import logging\n",
    "import scipy.io as sio\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, GlobalAveragePooling2D, Reshape,Flatten,Dense,add\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Lambda, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist, euclidean, cosine\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "from statistics import mean\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib as loss_metric\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers, n = np.load('details.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = np.load('training_samples.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_number = [len(i) for i in training_samples]\n",
    "total_samples = sum(samples_number)\n",
    "# # Draw the plot\n",
    "# plt.hist(samples_number,bins = 20,color = '#2F4F4F', edgecolor = 'w')    \n",
    "# # Title and labels\n",
    "# plt.title('Number of samples for each speaker')\n",
    "# plt.xlabel('Number of samples', size = 11)\n",
    "# plt.ylabel('Frequency', size= 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplets(data):\n",
    "    data_copy_1 = copy.deepcopy(data)\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    triplets = []\n",
    "    for speaker in range(n):\n",
    "        for audio in data_copy_1[speaker]:\n",
    "            data_copy = copy.deepcopy(data)\n",
    "            triplet = [audio]\n",
    "            data_copy[speaker].remove(audio)\n",
    "            random.shuffle(data_copy[speaker])\n",
    "            triplet.append(data_copy[speaker].pop(0))\n",
    "            del data_copy[speaker]\n",
    "            negative_id = random.randint(0, len(data_copy)-1)\n",
    "            random.shuffle(data_copy[negative_id])\n",
    "            triplet.append(data_copy[negative_id].pop(0))\n",
    "            triplets.append(triplet)\n",
    "        \n",
    "    return triplets\n",
    "    \n",
    "label_data = copy.deepcopy(training_samples)\n",
    "triplets_data = generate_triplets(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = 0\n",
    "for i in triplets_data:\n",
    "    if (i[0][1] == i[1][1]) and (i[0][1] != i[2][1]):\n",
    "        check +=1\n",
    "print(total_samples == len(triplets_data) == check)\n",
    "total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 16000\n",
    "PREEMPHASIS_ALPHA = 0.97\n",
    "FRAME_LEN = 0.025\n",
    "FRAME_STEP = 0.01\n",
    "NUM_FFT = 512\n",
    "BUCKET_STEP = 1\n",
    "MAX_SEC = 3\n",
    "\n",
    "WEIGHTS_FILE = \"SavedModel/weights.h5\"\n",
    "# COST_METRIC = \"cosine\" \n",
    "INPUT_SHAPE=(NUM_FFT,300,1)\n",
    "ROUNDS=2\n",
    "\n",
    "DURATION = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of Audio Data (Speech to Spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jameslyons/python_speech_features\n",
    "# This file includes routines for basic signal processing including framing and computing power spectra.\n",
    "# Author: James Lyons 2012\n",
    "\n",
    "\n",
    "def load_wav(filename, sample_rate):\n",
    "    audio, sr = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    audio_length = librosa.get_duration(audio,sr=sr)\n",
    "    DURATION.append(audio_length)\n",
    "    audio = audio.flatten()    \n",
    "    return audio\n",
    "\n",
    "# https://github.com/christianvazquez7/ivector/blob/master/MSRIT/rm_dc_n_dither.m\n",
    "def remove_dc_and_dither(sin, sample_rate):\n",
    "    if sample_rate == 16e3:\n",
    "        alpha = 0.99\n",
    "    elif sample_rate == 8e3:\n",
    "        alpha = 0.999\n",
    "    sin = lfilter([1,-1], [1,-alpha], sin)\n",
    "    dither = np.random.random_sample(len(sin)) + np.random.random_sample(len(sin)) - 1\n",
    "    spow = np.std(dither)\n",
    "    sout = sin + 1e-6 * spow * dither\n",
    "    return sout\n",
    "\n",
    "def preemphasis(signal, coeff=0.95):\n",
    "    return np.append(signal[0], signal[1:] - coeff * signal[:-1])\n",
    "\n",
    "def round_half_up(number):\n",
    "    return int(decimal.Decimal(number).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_UP))\n",
    "\n",
    "def rolling_window(a, window, step=1):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::step]\n",
    "\n",
    "\n",
    "def framesig(sig, frame_len, frame_step, winfunc=lambda x: np.ones((x,)), stride_trick=True):\n",
    "    \n",
    "    slen = len(sig)\n",
    "    frame_len = int(round_half_up(frame_len))\n",
    "    frame_step = int(round_half_up(frame_step))\n",
    "    if slen <= frame_len:\n",
    "        numframes = 1\n",
    "    else:\n",
    "        numframes = 1 + int(math.ceil((1.0 * slen - frame_len) / frame_step)) # LV\n",
    "\n",
    "    padlen = int((numframes - 1) * frame_step + frame_len)\n",
    "\n",
    "    zeros = np.zeros((padlen - slen,))\n",
    "    padsignal = np.concatenate((sig, zeros))\n",
    "    win = winfunc(frame_len)\n",
    "    frames = rolling_window(padsignal, window=frame_len, step=frame_step)\n",
    "    \n",
    "    return frames * win\n",
    "\n",
    "\n",
    "def normalize_frames(m,epsilon=1e-12):\n",
    "    return np.array([(v - np.mean(v)) / max(np.std(v),epsilon) for v in m])\n",
    "\n",
    "def build_buckets(max_sec, step_sec, frame_step):\n",
    "    buckets = {}\n",
    "    frames_per_sec = int(1/frame_step)\n",
    "    end_frame = int(max_sec*frames_per_sec)\n",
    "    step_frame = int(step_sec*frames_per_sec)\n",
    "    for i in range(0, end_frame+1, step_frame):\n",
    "        s = i\n",
    "        s = np.floor((s-7+2)/2) + 1  # conv1\n",
    "        s = np.floor((s-3)/2) + 1  # mpool1\n",
    "        s = np.floor((s-5+2)/2) + 1  # conv2\n",
    "        s = np.floor((s-3)/2) + 1  # mpool2\n",
    "        s = np.floor((s-3+2)/1) + 1  # conv3\n",
    "        s = np.floor((s-3+2)/1) + 1  # conv4\n",
    "        s = np.floor((s-3+2)/1) + 1  # conv5\n",
    "        s = np.floor((s-3)/2) + 1  # mpool5\n",
    "        s = np.floor((s-1)/1) + 1  # fc6\n",
    "        s = np.floor((s-1)/1) + 1  \n",
    "        s = np.floor((s-1)/1) + 1  \n",
    "        s = np.floor((s-1)/1) + 1  \n",
    "        if s > 0:\n",
    "            buckets[i] = int(s)\n",
    "    return buckets\n",
    "\n",
    "def get_fft_spectrum(filename):\n",
    "    buckets = build_buckets(MAX_SEC, BUCKET_STEP, FRAME_STEP)\n",
    "    signal = load_wav(filename,SAMPLE_RATE)\n",
    "    signal *= 2**15\n",
    "\n",
    "    # get FFT spectrum\n",
    "    signal = remove_dc_and_dither(signal, SAMPLE_RATE)\n",
    "    signal = preemphasis(signal, coeff=PREEMPHASIS_ALPHA)\n",
    "    frames = framesig(signal, frame_len=FRAME_LEN*SAMPLE_RATE, frame_step=FRAME_STEP*SAMPLE_RATE, winfunc=np.hamming)\n",
    "    fft = abs(np.fft.fft(frames,n=NUM_FFT))\n",
    "    fft_norm = normalize_frames(fft.T)\n",
    "\n",
    "    # truncate to max bucket sizes\n",
    "    rsize = max(k for k in buckets if k <= fft_norm.shape[1])\n",
    "    rstart = int((fft_norm.shape[1]-rsize)/2)\n",
    "    out = fft_norm[:,rstart:rstart+rsize]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGGModel(INP):\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(INP)\n",
    "    x = Conv2D(filters=96,kernel_size=(7,7), strides=(2,2), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2))(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    "    x = Conv2D(filters=256,kernel_size=(5,5), strides=(2,2), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(3,3),strides=(2,2))(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    "    x = Conv2D(filters=384,kernel_size=(3,3), strides=(1,1), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    "    x = Conv2D(filters=256,kernel_size=(3,3), strides=(1,1), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    "    x = Conv2D(filters=256,kernel_size=(3,3), strides=(1,1), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(0,0))(x)\n",
    "    x = Conv2D(filters=4096,kernel_size=(9,1), strides=(1,1), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1,1,4096))(x)\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(x)\n",
    "    x = Conv2D(filters=1024,kernel_size=(1,1), strides=(1,1), padding='valid')(x)\n",
    "    x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=None)(x)\n",
    "\n",
    "    x = Lambda(lambda y: K.l2_normalize(y, axis=3))(x)\n",
    "    x = Conv2D(filters=1024,kernel_size=(1,1), strides=(1,1), padding='valid')(x)\n",
    "    \n",
    "    MODEL = Model(INP, x, name='VGGModel')\n",
    "    return MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inp):\n",
    "    \n",
    "    base_model = VGGModel(inp)\n",
    "    base_model.load_weights(WEIGHTS_FILE)\n",
    "\n",
    "    poppedModel = Model(base_model.input,base_model.layers[-8].output)\n",
    "    # for i,layer in enumerate(poppedModel.layers):\n",
    "    #     print(i,layer.name)\n",
    "    for layer in poppedModel.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "    x = ZeroPadding2D(padding=(1,1))(poppedModel.layers[-1].output)\n",
    "    x = Conv2D(filters=2048,kernel_size=(1,1), strides=(1,1), padding='valid')(x)\n",
    "    # x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    # x = Activation('relu')(x) ,kernel_initializer=\"uniform\"\n",
    "    # x = MaxPooling2D(pool_size=(2,2),strides=None)(x)\n",
    "\n",
    "\n",
    "    # x = ZeroPadding2D(padding=(1,1))(poppedModel.layers[-1].output)\n",
    "    x = Conv2D(filters=1024,kernel_size=(1,1), strides=(1,1), padding='valid')(x)\n",
    "    # x = BatchNormalization(epsilon=1e-5,momentum=1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2),strides=None)(x)\n",
    "\n",
    "    x = Lambda(lambda y: K.l2_normalize(y, axis=3))(x)\n",
    "    x = Conv2D(filters=1024,kernel_size=(1,1), strides=(1,1), padding='valid')(x)\n",
    "\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(encoder,input_shape_1,input_shape_2,input_shape_3):\n",
    "    \n",
    "    input_1 = Input(input_shape_1)\n",
    "    input_2 = Input(input_shape_2)\n",
    "    input_3 = Input(input_shape_3)\n",
    "    \n",
    "    anchor = encoder(input_1)\n",
    "    positive = encoder(input_2)\n",
    "    negative = encoder(input_3)\n",
    "    print('Encoder built!')\n",
    "    \n",
    "    model_ = Model(inputs=[input_1, input_2,input_3], outputs=[anchor,positive,negative])\n",
    "\n",
    "    return model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true,y_pred,alpha=0.2):\n",
    "\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)), axis = -1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)), axis = -1)\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = tf.reduce_sum(pos_dist - neg_dist + alpha)\n",
    "#     loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "\n",
    "    return loss  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\lenovo\\desktop\\speakeridentification\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Encoder built!\n"
     ]
    }
   ],
   "source": [
    "triplet_loss_model = build_network(encoder,INPUT_SHAPE,INPUT_SHAPE,INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_loss_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilation\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.01)\n",
    "triplet_loss_model.compile(optimizer =opt,loss=triplet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 2)\n",
      "(75, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleName</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_train/id10003.1.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>[[-0.5712448551183048, -0.6264123611076122, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_train/id10003.19.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>[[0.9372785176525272, 0.6402701485044744, -0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_train/id10003.63.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>[[0.6956728562522385, 0.20181905876285386, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_train/id10003.68.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>[[-0.0506585389249207, -0.9657955561847444, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_train/id10003.70.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>[[0.3998710348647563, -0.37867003648834, -0.57...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SampleName  Speaker  \\\n",
       "0   data_train/id10003.1.wav  id10003   \n",
       "1  data_train/id10003.19.wav  id10003   \n",
       "2  data_train/id10003.63.wav  id10003   \n",
       "3  data_train/id10003.68.wav  id10003   \n",
       "4  data_train/id10003.70.wav  id10003   \n",
       "\n",
       "                                           Embedding  \n",
       "0  [[-0.5712448551183048, -0.6264123611076122, -0...  \n",
       "1  [[0.9372785176525272, 0.6402701485044744, -0.4...  \n",
       "2  [[0.6956728562522385, 0.20181905876285386, -0....  \n",
       "3  [[-0.0506585389249207, -0.9657955561847444, -0...  \n",
       "4  [[0.3998710348647563, -0.37867003648834, -0.57...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df = pd.read_csv('train.csv')\n",
    "print(embeddings_df.shape)\n",
    "embeddings_df['Embedding'] = embeddings_df['SampleName'].apply(lambda x: get_fft_spectrum(x))\n",
    "print(embeddings_df.shape)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_header = ['ID_anchor','emb_anchor','ID_positive','emb_positive','ID_negative','emb_negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_embeddings(triplets):\n",
    "    with open('train_triplet_loss.csv', 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=train_header)\n",
    "        writer.writeheader()\n",
    "        for triplet in triplets:\n",
    "            row = []\n",
    "            row.append(triplet[0][1])\n",
    "            row.append(triplet[0][0])\n",
    "            row.append(triplet[1][1])\n",
    "            row.append(triplet[1][0])\n",
    "            row.append(triplet[2][1])\n",
    "            row.append(triplet[2][0])            \n",
    "            writer.writerow(dict(zip(train_header, row)))\n",
    "            \n",
    "    train_df = pd.read_csv('train_triplet_loss.csv')    \n",
    "    return train_df['emb_anchor'],train_df['emb_positive'],train_df['emb_negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A,X_P,X_N = get_audio_embeddings(triplets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_anchor</th>\n",
       "      <th>emb_anchor</th>\n",
       "      <th>ID_positive</th>\n",
       "      <th>emb_positive</th>\n",
       "      <th>ID_negative</th>\n",
       "      <th>emb_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.1.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.111.wav</td>\n",
       "      <td>id10005</td>\n",
       "      <td>data_train/id10005.30.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.19.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.70.wav</td>\n",
       "      <td>id10005</td>\n",
       "      <td>data_train/id10005.4.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.63.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.70.wav</td>\n",
       "      <td>id10005</td>\n",
       "      <td>data_train/id10005.2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.68.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.72.wav</td>\n",
       "      <td>id10007</td>\n",
       "      <td>data_train/id10007.19.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.70.wav</td>\n",
       "      <td>id10003</td>\n",
       "      <td>data_train/id10003.136.wav</td>\n",
       "      <td>id10006</td>\n",
       "      <td>data_train/id10006.64.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_anchor                 emb_anchor ID_positive  \\\n",
       "0   id10003   data_train/id10003.1.wav     id10003   \n",
       "1   id10003  data_train/id10003.19.wav     id10003   \n",
       "2   id10003  data_train/id10003.63.wav     id10003   \n",
       "3   id10003  data_train/id10003.68.wav     id10003   \n",
       "4   id10003  data_train/id10003.70.wav     id10003   \n",
       "\n",
       "                 emb_positive ID_negative               emb_negative  \n",
       "0  data_train/id10003.111.wav     id10005  data_train/id10005.30.wav  \n",
       "1   data_train/id10003.70.wav     id10005   data_train/id10005.4.wav  \n",
       "2   data_train/id10003.70.wav     id10005   data_train/id10005.2.wav  \n",
       "3   data_train/id10003.72.wav     id10007  data_train/id10007.19.wav  \n",
       "4  data_train/id10003.136.wav     id10006  data_train/id10006.64.wav  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_triplet_loss.csv')    \n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_len = len(triplets_data)\n",
    "Y = [np.random.rand(1024,).reshape(-1,1024,) for i in range(triplet_len)]\n",
    "y = [np.array(Y),np.array(Y),np.array(Y)]\n",
    "buckets = build_buckets(MAX_SEC, BUCKET_STEP, FRAME_STEP)\n",
    "X_A_emb = list(train_df['emb_anchor'].map(embeddings_df.set_index('SampleName')['Embedding']))\n",
    "X_P_emb = list(train_df['emb_positive'].map(embeddings_df.set_index('SampleName')['Embedding']))\n",
    "X_N_emb = list(train_df['emb_negative'].map(embeddings_df.set_index('SampleName')['Embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(DURATION,bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Duration: 4.0000625    Avg Duration: 8.404329166666669    Max Duration: 34.0000625\n"
     ]
    }
   ],
   "source": [
    "print('Min Duration:',min(DURATION),'   Avg Duration:',sum(DURATION)/len(DURATION),'   Max Duration:',max(DURATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "139/139 [==============================] - ETA: 3:19 - loss: 0.5533 - conv2d_11_loss: 0.1978 - conv2d_22_loss: 0.1990 - conv2d_33_loss: 0.15 - ETA: 2:51 - loss: 0.5883 - conv2d_11_loss: 0.1988 - conv2d_22_loss: 0.2018 - conv2d_33_loss: 0.18 - ETA: 2:13 - loss: 0.6043 - conv2d_11_loss: 0.2024 - conv2d_22_loss: 0.2117 - conv2d_33_loss: 0.19 - ETA: 1:38 - loss: 0.6037 - conv2d_11_loss: 0.2014 - conv2d_22_loss: 0.2085 - conv2d_33_loss: 0.19 - ETA: 1:04 - loss: 0.5816 - conv2d_11_loss: 0.2028 - conv2d_22_loss: 0.1853 - conv2d_33_loss: 0.19 - ETA: 32s - loss: 0.5841 - conv2d_11_loss: 0.2011 - conv2d_22_loss: 0.1876 - conv2d_33_loss: 0.1954 - 241s 2s/step - loss: 0.5953 - conv2d_11_loss: 0.2008 - conv2d_22_loss: 0.1993 - conv2d_33_loss: 0.1952\n",
      "Epoch 2/4\n",
      " 20/139 [===>..........................] - ETA: 3:33 - loss: 0.2775 - conv2d_11_loss: 0.2018 - conv2d_22_loss: -0.1442 - conv2d_33_loss: 0.2199"
     ]
    }
   ],
   "source": [
    "triplet_loss_model.fit([np.array(X_A_emb).reshape(-1,512,300,1), np.array(X_P_emb).reshape(-1,512,300,1), np.array(X_N_emb).reshape(-1,512,300,1)],[y[0],y[1],y[2]],epochs=4,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss_model.save('try_model_2.h5')\n",
    "triplet_loss_model.save_weights('tlm_try_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_training_data(speaker, prediction):\n",
    "    data_val = embeddings_df[['SampleName', 'Speaker','Embedding']]\n",
    "    size = len(speaker)\n",
    "    A_VAL = []\n",
    "    P_VAL = []\n",
    "    N_VAL = []\n",
    "    for i in range(size):\n",
    "        if speaker[i] != prediction[i]:\n",
    "            for _ in range(2):\n",
    "                [a,p] = data_val[data_val['Speaker'] == speaker[i]]['Embedding'].sample(2).tolist()\n",
    "                [n] = data_val[data_val['Speaker'] == prediction[i]]['Embedding'].sample(1).tolist()\n",
    "                A_VAL.append(a)\n",
    "                P_VAL.append(p)\n",
    "                N_VAL.append(n) \n",
    "    return A_VAL, P_VAL, N_VAL\n",
    "\n",
    "\n",
    "\n",
    "def convert_emb_to_vec_train(model, embeddings_df):\n",
    "    embeddings_df['Vector'] = embeddings_df['Embedding'].apply(lambda x: np.squeeze(encoder_trained.predict(np.array(x).reshape(1,512,300,1))))\n",
    "    return embeddings_df[['SampleName','Speaker','Vector']]\n",
    "\n",
    "\n",
    "def convert_emb_to_vec(model, df):\n",
    "    df['Embedding'] = df['SampleName'].apply(lambda x: get_fft_spectrum(x))\n",
    "    df['Vector'] = df['Embedding'].apply(lambda x: np.squeeze(encoder_trained.predict(np.array(x).reshape(1,*np.array(x).shape,1))))\n",
    "    return df[['SampleName','Speaker','Vector']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for roundno in range(ROUNDS):\n",
    "    print('VALIDATION AND TESTING ROUND',roundno+1,'STARTED!')\n",
    "\n",
    "    print('Loading trained Model....')\n",
    "    try_model = load_model('try_model_2.h5', custom_objects={'triplet_loss':triplet_loss})\n",
    "    try_model.load_weights('tlm_try_weights.h5')\n",
    "\n",
    "\n",
    "    print('Extracting encoder from trained Model...')\n",
    "    encoder_trained = Model(try_model.layers[0].input,try_model.layers[-3].output)\n",
    "\n",
    "\n",
    "    print('Processing train samples...')\n",
    "    train_result = convert_emb_to_vec_train(encoder_trained, embeddings_df)\n",
    "    trained_vec = np.array([vec.tolist() for vec in train_result['Vector']])\n",
    "    speakers = train_result['Speaker']\n",
    "\n",
    "\n",
    "    print('\\nProcessing Validation samples...')\n",
    "    val_df = pd.read_csv('validation.csv')\n",
    "    val_result = convert_emb_to_vec(encoder_trained, val_df)\n",
    "    val_vec= np.array([vec.tolist() for vec in val_result['Vector']])\n",
    "\n",
    "    print(\"Comparing validation samples against trained samples....\")\n",
    "    distances = pd.DataFrame(cdist(val_vec, trained_vec, metric='euclidean'), columns=speakers)\n",
    "    val_df = pd.concat([val_df, distances],axis=1)\n",
    "    val_df['PredictedSpeaker'] = val_df[speakers].idxmin(axis=1)\n",
    "    val_df['Correct'] = (val_df['PredictedSpeaker'] == val_df['Speaker'])*1. # bool to int\n",
    "    print('\\nNumber of validation samples: ', val_df.shape[0])\n",
    "    print('Number of correctly predicted speakers: ', sum(val_df['Correct'].tolist()))\n",
    "    validation_accuracy = sum(val_df['Correct'].tolist())/ val_df.shape[0]\n",
    "    print('Validation Accuracy:',validation_accuracy)\n",
    "    val_acc.append(validation_accuracy)\n",
    "\n",
    "    print('\\nAdding triplets...')\n",
    "    result = val_df[['SampleName','Speaker','PredictedSpeaker', 'Correct']]\n",
    "    speaker_val = result['Speaker'].tolist()\n",
    "    prediction_val = result['PredictedSpeaker'].tolist()\n",
    "\n",
    "    Anchor,Positive,Negative= recreate_training_data(speaker_val, prediction_val)\n",
    "\n",
    "    X_A_emb = X_A_emb + Anchor\n",
    "    X_P_emb = X_P_emb + Positive\n",
    "    X_N_emb = X_N_emb + Negative\n",
    "    data_size = len(X_A_emb)\n",
    "    Y = [np.random.rand(1024,).reshape(-1,1024,) for i in range(data_size)]\n",
    "    y = [np.array(Y),np.array(Y),np.array(Y)]\n",
    "\n",
    "    print('Retraining the model...')\n",
    "    triplet_loss_model.fit([np.array(X_A_emb).reshape(-1,512,300,1), np.array(X_P_emb).reshape(-1,512,300,1), np.array(X_N_emb).reshape(-1,512,300,1)],[y[0],y[1],y[2]],epochs=2,batch_size=28)\n",
    "    print('\\nSaving the retrained Model...')\n",
    "    triplet_loss_model.save('try_model_2.h5')\n",
    "    triplet_loss_model.save_weights('tlm_try_weights.h5')\n",
    "\n",
    "    print('\\nProcessing Testing samples...')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    test_result = convert_emb_to_vec(encoder_trained, test_df)\n",
    "    test_vec= np.array([vec.tolist() for vec in test_result['Vector']])\n",
    "\n",
    "    print(\"Comparing test samples against trained samples....\")\n",
    "    distances = pd.DataFrame(cdist(test_vec, trained_vec, metric='euclidean'), columns=speakers)\n",
    "    test_df = pd.concat([test_df, distances],axis=1)\n",
    "    test_df['PredictedSpeaker'] = test_df[speakers].idxmin(axis=1)\n",
    "    test_df['Correct'] = (test_df['PredictedSpeaker'] == test_df['Speaker'])*1. # bool to int\n",
    "    print('\\nNumber of test samples: ', test_df.shape[0])\n",
    "    print('Number of correctly predicted speakers: ', sum(test_df['Correct'].tolist()))\n",
    "    test_accuracy = sum(test_df['Correct'].tolist())/ test_df.shape[0]\n",
    "    print('Test Accuracy:',test_accuracy)\n",
    "    test_acc.append(test_accuracy)\n",
    "\n",
    "    print('VALIDATION AND TESTING ROUND',roundno+1,'COMPLETED!\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading trained Model....')\n",
    "try_model = load_model('try_model_2.h5', custom_objects={'triplet_loss':triplet_loss})\n",
    "try_model.load_weights('tlm_try_weights.h5')\n",
    "\n",
    "print('Extracting encoder from trained Model...')\n",
    "encoder_trained = Model(try_model.layers[0].input,try_model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading New Data...')\n",
    "newdata = pd.read_csv('newdata.csv')\n",
    "newdata['Embedding'] = newdata['SampleName'].apply(lambda x: get_fft_spectrum(x))\n",
    "newdata['Vector'] = newdata['Embedding'].apply(lambda x: np.squeeze(encoder_trained.predict(np.array(x).reshape(1,512,300,1))))\n",
    "\n",
    "print('Adding New Data...')\n",
    "newemb = [embeddings_df, newdata]\n",
    "new_embeddings_df = pd.concat(newemb)\n",
    "new_embeddings_df.shape[0] == embeddings_df.shape[0] + newdata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Audio Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample(sample, embeddings_df):\n",
    "\n",
    "    embedding = get_fft_spectrum(sample)\n",
    "    vector = np.squeeze(encoder_trained.predict(np.array(embedding).reshape(1,512,300,1)))\n",
    "    \n",
    "    embeddings_df['distance'] = embeddings_df['Vector'].apply(lambda x: scipy.spatial.distance.euclidean(x, vector))\n",
    "    sample_id = embeddings_df.loc[embeddings_df['distance'].idxmin()]\n",
    "    print(embeddings_df['distance'].tolist())\n",
    "    embeddings_df = embeddings_df.drop(columns=['distance'])\n",
    "    print(sample_id['Speaker'])\n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2427046298980713, 1.802146077156067, 2.125948667526245, 1.8229680061340332, 1.9165761470794678, 1.9137336015701294, 2.0627448558807373, 1.8275219202041626, 1.8857040405273438, 1.8514316082000732, 2.3079757690429688, 2.101149082183838, 1.954125165939331, 2.170279026031494, 1.8406250476837158, 1.9186877012252808, 1.9977394342422485, 1.839442491531372, 2.0090396404266357, 1.9637689590454102, 1.6776297092437744, 1.8362914323806763, 1.7075445652008057, 1.7793054580688477, 1.8177361488342285, 1.661380648612976, 1.7806835174560547, 1.7464728355407715, 1.901174545288086, 2.160890817642212, 1.8956378698349, 1.8700095415115356, 1.8781826496124268, 2.110664129257202, 1.9521498680114746, 1.8839738368988037, 2.1610114574432373, 2.0599677562713623, 1.874345064163208, 2.2109084129333496, 1.8703500032424927, 1.7943518161773682, 1.9382296800613403, 1.8340152502059937, 1.905133605003357, 1.8149921894073486, 1.7418516874313354, 2.097632884979248, 2.038558006286621, 2.036163330078125, 1.7829827070236206, 1.8865814208984375, 1.7769695520401, 2.0503833293914795, 1.6356308460235596, 1.6887097358703613, 1.652355670928955, 1.8094673156738281, 2.101667881011963, 1.820654273033142, 2.094313383102417, 2.2372376918792725, 2.2305262088775635, 2.042762041091919, 2.232116460800171, 2.210052251815796, 2.104513168334961, 2.0349931716918945, 2.3369786739349365, 2.3008651733398438, 2.273099184036255, 2.066740036010742, 2.4173998832702637, 2.342895030975342, 1.9496076107025146]\n",
      "id10006\n"
     ]
    }
   ],
   "source": [
    "embeddings_df = test_sample('data_test/id10003.83.wav', embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
